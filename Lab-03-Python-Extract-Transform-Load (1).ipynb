{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cf167a",
   "metadata": {},
   "source": [
    "## Using Python to Perform Extract-Transform-Load (ETL Processing)\n",
    "Modern Data Warehousing and Analytics solutions frequently use languages like Python or Scala to extract data from numerous sources, including relational database management systems, NoSQL database systems, real-time streaming endpoints and Data Lakes.  These languages can then be used to perform many types of transformation before then loading the data into a variety of destinations including file systems and data warehouses. This data can then be consumed by data scientists or business analysts.\n",
    "\n",
    "In this lab you will recreate the **Northwind_DW** dimensional database from Lab 2; however, you'll take an entirely different approach. Instead of extracting, transforming and loading the date entirely on the database system entirely using SQL data definition language (DDL) and data manipulation language (DML) statements, here you will learn to interact with the RDBMS from a remote client running Python. You will learn to fetch data into Pandas DataFrames, perform all the necessary transformations in-memory on the client, and then push the newly transformed DataFrame back to the RDBMS using a Pandas function that will create the table and fill it with data with a single operation.\n",
    "\n",
    "### Prerequisites:\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7fe77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31522597",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = \"localhost\"\n",
    "host_ip = \"127.0.0.1\"\n",
    "port = \"3306\"\n",
    "user_id = \"root\"\n",
    "pwd = \"Passw0rd123\"\n",
    "\n",
    "src_dbname = \"northwind\"\n",
    "dst_dbname = \"northwind_dw2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42abc",
   "metadata": {},
   "source": [
    "#### Define Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9773",
   "metadata": {},
   "source": [
    "#### Create the New Data Warehouse database, and to Use it, Switch the Connection Context.\n",
    "Clearly, you won't get very far without having a database to work with. Here we demonstrate how we can *drop* a database if it already exists, and then *create* the new **northwind_dw2** database and *use* it as the target of all subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70548734",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "sqlEngine.execute(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"CREATE DATABASE `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"USE {dst_dbname};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02169776",
   "metadata": {},
   "source": [
    "### 1.0. Create & Populate the Dimension Tables\n",
    "#### 1.1. Extract Data from the Source Database Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbe3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_customers = \"SELECT * FROM northwind.customers;\"\n",
    "df_customers = get_dataframe(user_id, pwd, host_name, src_dbname, sql_customers)\n",
    "df_customers.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a278a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_employees = \"SELECT * FROM northwind.employees;\"\n",
    "df_employees = get_dataframe(user_id, pwd, host_name, src_dbname, sql_employees)\n",
    "df_employees.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8da1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_products = \"SELECT * FROM northwind.products;\"\n",
    "df_products = get_dataframe(user_id, pwd, host_name, src_dbname, sql_products)\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a13566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_shippers = \"SELECT * FROM northwind.shippers;\"\n",
    "df_shippers = get_dataframe(user_id, pwd, host_name, src_dbname, sql_shippers)\n",
    "df_shippers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900ab3f",
   "metadata": {},
   "source": [
    "#### 1.2. Perform Any Necessary Transformations\n",
    "Pandas DataFrames enable extensive data modification capabilities. Here we will start by simply dropping features (columns) that we don't believe provide any real value to our analytics solution. Examples include columns having a high percentage of NULL values, columns having large amounts of free-text, and columns having binary large object (BLOB) data such as images or other documents. Then, we will rename the primary key column (id) to conform with data warehouse design standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['email_address','home_phone','mobile_phone','web_page','notes','attachments']\n",
    "df_customers.drop(drop_cols, axis=1, inplace=True)\n",
    "df_customers.rename(columns={\"id\":\"customer_key\"}, inplace=True)\n",
    "\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a618aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['mobile_phone','notes','attachments']\n",
    "df_employees.drop(drop_cols, axis=1, inplace=True)\n",
    "df_employees.rename(columns={\"id\":\"employee_key\"}, inplace=True)\n",
    "\n",
    "df_employees.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['supplier_ids','description','attachments']\n",
    "df_products.drop(drop_cols, axis=1, inplace=True)\n",
    "df_products.rename(columns={\"id\":\"product_key\"}, inplace=True)\n",
    "\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['last_name','first_name','email_address','job_title','business_phone',\n",
    "             'home_phone','mobile_phone','fax_number','web_page','notes','attachments']\n",
    "df_shippers.drop(drop_cols, axis=1, inplace=True)\n",
    "df_shippers.rename(columns={\"id\":\"shipper_key\"}, inplace=True)\n",
    "\n",
    "df_shippers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cecaeb",
   "metadata": {},
   "source": [
    "#### 1.4. Load the Transformed DataFrames into the New Data Warehouse by Creating New Tables\n",
    "Here I demonstrate how we can create an iterable data structure containing the values needed to correctly create and populate the new dimension tables. If you inspect this code listing carefully, you'll notice that it's a **list** containing a **set** *(or vector)* for each dimension table. Each **set** then contains the *table_name* we need to assign to the table, the *pandas DataFrame* we crafted to define & populate the table, and the name we need to assign to the *primary_key* column.  With this *list of sets* defined, we can then call our **set_dataframe( )** function from within a **for *loop*** to create each *dimension* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b84457",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_operation = \"insert\"\n",
    "\n",
    "tables = [('dim_customers', df_customers, 'customer_key'),\n",
    "          ('dim_employees', df_employees, 'employee_key'),\n",
    "          ('dim_products', df_products, 'product_key'),\n",
    "          ('dim_shippers', df_shippers, 'shipper_key')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dc940",
   "metadata": {},
   "source": [
    "### 2.0. Create & Populate the Fact Table\n",
    "Here we will learn two approaches to creating the *fact_orders* fact table. The first approach demonstrates that a carefully crafted SQL SELECT statement can be used to perform this task... *but what fun would that be.* Seriously though, this approach is quick and effect if you already have the query, but what if you didn't have the opportunity to view and work with the data beforehand?  What's more, you may be required to combine data from multiple sources, some of which may not be relational database management systems. Then, a simple SQL query won't do!  You would need to load the data from the various sources (e.g., database tables, CSV or JSON files, NoSQL document collections, API stream data) and then combine them into a single dataframe that you could then use to create a new database table. For this reason we'll see how we can retrieve the data, but we won't bother to use it for creating a new table... we already know how to do that using the **set_dataframe( )** function anyway.\n",
    "\n",
    "#### 2.1. First, you could simply use the SQL SELECT statement you authored in Lab 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ff332",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_fact_orders = \"\"\"\n",
    "SELECT o.id,\n",
    "\to.employee_id,\n",
    "    o.customer_id,\n",
    "    od.product_id,\n",
    "    o.shipper_id,\n",
    "    o.ship_name,\n",
    "    o.ship_address,\n",
    "    o.ship_city,\n",
    "    o.ship_state_province,\n",
    "    o.ship_zip_postal_code,\n",
    "    o.ship_country_region,\n",
    "    od.quantity,\n",
    "    o.order_date,\n",
    "    o.shipped_date,\n",
    "    od.unit_price,\n",
    "    od.discount,\n",
    "    o.shipping_fee,\n",
    "    o.taxes,\n",
    "    o.payment_type,\n",
    "    o.paid_date,\n",
    "    o.tax_rate,\n",
    "    os.status_name AS order_status,\n",
    "    ods.status_name AS order_details_status\n",
    "FROM northwind.orders AS o\n",
    "INNER JOIN northwind.orders_status AS os\n",
    "ON o.status_id = os.id\n",
    "RIGHT OUTER JOIN northwind.order_details AS od\n",
    "ON o.id = od.order_id\n",
    "INNER JOIN northwind.order_details_status AS ods\n",
    "ON od.status_id = ods.id;\n",
    "\"\"\"\n",
    "\n",
    "df_fact_orders = get_dataframe(user_id, pwd, host_name, src_dbname, sql_fact_orders)\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739c0ad",
   "metadata": {},
   "source": [
    "#### 2.2. Instead, Implement the solution using Pandas DataFrames to Craft the table\n",
    "This is where we get to the point of this lab.  We'll query the source **northwind** database to fill a *dataframe* for each of the source tables we need to create our *fact_orders* fact table; orders, orders_status, order_details and order_details_status. Then, we'll learn how to *join* those *dataframes* using the **merge( )** method of the Pandas DataFrame.  We'll make any additional changes that we expect to see reflected in the *fact* table in our new MySQL database, and then, we'll push the *dataframe* back to the MySQL server to create and populate the new *fact* table.\n",
    "\n",
    "##### 2.2.1. Get all the data from each of the four tables involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17596425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_orders = \"SELECT * FROM northwind.orders;\"\n",
    "df_orders = get_dataframe(user_id, pwd, host_name, src_dbname, sql_orders)\n",
    "df_orders.rename(columns={\"id\":\"order_id\"}, inplace=True)\n",
    "df_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b138ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_orders_status = \"SELECT * FROM northwind.orders_status;\"\n",
    "df_orders_status = get_dataframe(user_id, pwd, host_name, src_dbname, sql_orders_status)\n",
    "df_orders_status.rename(columns={\"id\":\"status_id\"}, inplace=True)\n",
    "df_orders_status.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_order_details = \"SELECT * FROM northwind.order_details;\"\n",
    "df_order_details = get_dataframe(user_id, pwd, host_name, src_dbname, sql_order_details)\n",
    "df_order_details.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_order_details_status = \"SELECT * FROM northwind.order_details_status;\"\n",
    "df_order_details_status = get_dataframe(user_id, pwd, host_name, src_dbname, sql_order_details_status)\n",
    "df_order_details_status.rename(columns={\"id\":\"status_id\"}, inplace=True)\n",
    "df_order_details_status.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bacaed6",
   "metadata": {},
   "source": [
    "##### 2.2.2. Get the order_status column.\n",
    "Here we use the dataframe's **merge( )** method to **inner join** the *orders* and the *orders_status* dataframes **on** the *status_id* column. We then use the dataframe's **rename( )** method to rename the *status_name* column to *order_status*, and use the dataframe's **drop( )** method to remove the *status_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.merge(df_orders, df_orders_status, on='status_id', how='inner')\n",
    "df_orders.rename(columns={\"status_name\": \"order_status\"}, inplace=True)\n",
    "df_orders.drop(['status_id'], axis=1, inplace=True)\n",
    "df_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d2fb",
   "metadata": {},
   "source": [
    "##### 2.2.3. Get the order_details_status\n",
    "Here we repeat the sequence of operations we used in the previous step to **inner join** the *order_details* and *order_details_status* dataframes for the sake of including the *order_details_status* column in place of the *status_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ee5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_details = pd.merge(df_order_details, df_order_details_status, on='status_id', how='inner')\n",
    "df_order_details.rename(columns={\"status_name\": \"order_details_status\"}, inplace=True)\n",
    "df_order_details.drop(['status_id'], axis=1, inplace=True)\n",
    "df_order_details.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810fc64",
   "metadata": {},
   "source": [
    "##### 2.2.4. Join the Orders and OrderDetails DataFrames\n",
    "In this step we can now easily join the *orders* and *order_details* dataframes. Since each **order** (the *left* dataframe) can have many **order details** (the *right* dataframe), we'll need to implement a **right** *outer join* **on** the *order_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders = pd.merge(df_orders, df_order_details, on='order_id', how='right')\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d844e",
   "metadata": {},
   "source": [
    "##### 2.2.5. Perform any Additional Transformations\n",
    "In this step we can prepare the DataFrame so that it defines exactly what we want to see created in the database.  Issues may include dropping unwanted columns, reordering the columns, and in our case, creating a new column to serve as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['id', 'notes', 'tax_status_id', 'purchase_order_id', 'inventory_id', 'date_allocated']\n",
    "df_fact_orders.drop(drop_columns, axis=1, inplace=True)\n",
    "ordered_columns = ['order_id', 'employee_id', 'customer_id', 'product_id', 'shipper_id', 'ship_name',\n",
    "                  'ship_address', 'ship_city', 'ship_state_province', 'ship_zip_postal_code', \n",
    "                  'quantity', 'order_date', 'shipped_date', 'unit_price', 'discount', 'shipping_fee', 'taxes',\n",
    "                  'payment_type', 'paid_date', 'order_status', 'order_details_status']\n",
    "\n",
    "df_fact_orders = df_fact_orders[ordered_columns]\n",
    "df_fact_orders.insert(0, \"order_key\", range(1, df_fact_orders.shape[0]+1))\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd75dbc",
   "metadata": {},
   "source": [
    "##### 2.2.6. Write the DataFrame Back to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"fact_orders\"\n",
    "primary_key = \"order_key\"\n",
    "dp_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, df_fact_orders, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bac81",
   "metadata": {},
   "source": [
    "### 3.0. Demonstrate that the New Data Warehouse Exists and Contains the Correct Data\n",
    "To demonstrate the viability of your solution, author a SQL SELECT statement that returns:\n",
    "-\tEach Customerâ€™s Last Name\n",
    "-\tThe total amount of the order quantity associated with each customer\n",
    "-\tThe total amount of the order unit price associated with each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_test = \"\"\"\n",
    "    SELECT customers.`last_name` AS `customer_name` ,\n",
    "        SUM(orders.`quantity`) AS `total_quantity` ,\n",
    "        SUM(orders.`unit_price`) AS `total_unit_price` ,\n",
    "    FROM `northwind_dw2`.`fact_orders` AS orders\n",
    "    INNER JOIN `northwind_dw2`.`dim_customers` AS customers\n",
    "    ON orders.customer_id = customers.customer_key\n",
    "    GROUP BY customers.`last_name`\n",
    "    ORDER BY total_unit_price DESC;\n",
    "\"\"\".format(dst_dbname)\n",
    "\n",
    "df_test = get_dataframe(user_id, pwd, host_name, src_dbname, sql_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe196ec-a7d7-4f10-81fc-63826cc91309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
